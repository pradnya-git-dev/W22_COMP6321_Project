{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nz5gX7XkdGo8"
      },
      "source": [
        "1. Installing required modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0PWTVK1iaXKL"
      },
      "outputs": [],
      "source": [
        "# Installs required modules\n",
        "\n",
        "!pip install --upgrade watermark blackcellmagic\n",
        "!pip install datasets\n",
        "!pip install transformers\n",
        "!pip install umap-learn\n",
        "!pip install nlpaug\n",
        "!pip install \"ray[tune]\"\n",
        "!pip install hyperopt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IheQ3gZTdQET"
      },
      "source": [
        "2. Importing required modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Js-DLnUXa_RA"
      },
      "outputs": [],
      "source": [
        "# Loads Black extension and imports required modules\n",
        "%load_ext blackcellmagic\n",
        "\n",
        "import torch\n",
        "from datasets import load_dataset, DatasetDict, Features, Value, ClassLabel\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback\n",
        "from transformers import set_seed\n",
        "from umap import UMAP\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, ConfusionMatrixDisplay, confusion_matrix, classification_report\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from ray.tune.suggest.hyperopt import HyperOptSearch\n",
        "from ray.tune.schedulers import ASHAScheduler\n",
        "from ray import tune\n",
        "import json\n",
        "import os\n",
        "import shutil\n",
        "from joblib import dump, load\n",
        "set_seed(0)\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeqViK9HdT0o"
      },
      "source": [
        "3. Defining utility functions for the entire project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NlmT51ZJcCf_"
      },
      "outputs": [],
      "source": [
        "def prepare_dataset(working_dir=os.getcwd()):\n",
        "\n",
        "  train_file = working_dir + \"/train_dataset_processed.csv\"\n",
        "  test_file = working_dir + \"/test_dataset_processed.csv\"\n",
        "\n",
        "  data_files = {\"train\": train_file, \"test\": test_file}\n",
        "  class_names = [\"negative\", \"neutral\", \"positive\"]\n",
        "  data_columns = Features({\"label\": ClassLabel(names=class_names), \"text\": Value(\"string\")})\n",
        "  original_data = load_dataset(\"csv\", data_files=data_files, column_names=[\"label\", \"text\"], features=data_columns)\n",
        "\n",
        "  train_valid_data = original_data[\"train\"].train_test_split(test_size=0.2)\n",
        "  train_dataset = train_valid_data[\"train\"]\n",
        "  valid_dataset = train_valid_data[\"test\"]\n",
        "  project_dataset = DatasetDict({\n",
        "      \"train\": train_dataset,\n",
        "      \"valid\": valid_dataset,\n",
        "      \"test\": original_data[\"test\"]})\n",
        "\n",
        "  # Removing empty text entries\n",
        "  project_dataset = project_dataset.filter(lambda example: example[\"text\"] != None)\n",
        "  return project_dataset\n",
        "\n",
        "\n",
        "def plot_confusion_matrix(y_preds, y_true, labels, title):\n",
        "  cm = confusion_matrix(y_true, y_preds, normalize=\"true\")\n",
        "  fix, ax = plt.subplots(figsize=(6, 6))\n",
        "  cm_display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
        "  cm_display.plot(cmap=\"Blues\", values_format=\".2f\", ax=ax, colorbar=False)\n",
        "  plt.title(\"Normalized confusion matrix for \" + title)\n",
        "  plt.show()\n",
        "\n",
        "def print_classification_report(y_preds, y_true, title):\n",
        "  labels = [\"negative\", \"neutral\", \"positive\"]\n",
        "  plot_confusion_matrix(y_preds, y_true, labels, title)\n",
        "  print(\"\\n\\n\")\n",
        "  print(\"Classification report for \" + title)\n",
        "  mlp_report = classification_report(y_true, y_preds, target_names=labels)\n",
        "  print(\"-\" * 75)\n",
        "  print(mlp_report)\n",
        "  print(\"-\" * 75)\n",
        "\n",
        "def display_data_properties(dataset):\n",
        "  \"\"\"Checking the data distribution\"\"\"\n",
        "  # Using pandas for checking the data distribution\n",
        "  dataset.set_format(type=\"pandas\")\n",
        "  train_df = dataset[\"train\"][:]\n",
        "  valid_df = dataset[\"valid\"][:]\n",
        "  test_df = dataset[\"test\"][:]\n",
        "\n",
        "  # Printing the first 5 examples in the train data\n",
        "  def label_int2str(row):\n",
        "    return dataset[\"train\"].features[\"label\"].int2str(row)\n",
        "\n",
        "  train_df[\"label_name\"] = train_df[\"label\"].apply(label_int2str)\n",
        "  valid_df[\"label_name\"] = valid_df[\"label\"].apply(label_int2str)\n",
        "  test_df[\"label_name\"] = test_df[\"label\"].apply(label_int2str)\n",
        "  print(train_df.head())\n",
        "\n",
        "  # Displaying the frequency of classes in the training data\n",
        "  train_df[\"label_name\"].value_counts(ascending=True).plot.barh()\n",
        "  plt.title(\"Frequency of Classes in training data\")\n",
        "  plt.show()\n",
        "  plt.figure()\n",
        "  valid_df[\"label_name\"].value_counts(ascending=True).plot.barh()\n",
        "  plt.title(\"Frequency of Classes in validation data\")\n",
        "  plt.show()\n",
        "  plt.figure()\n",
        "  test_df[\"label_name\"].value_counts(ascending=True).plot.barh()\n",
        "  plt.title(\"Frequency of Classes in test data\")\n",
        "  plt.show()\n",
        "\n",
        "  # Resetting the dataset format\n",
        "  dataset.reset_format()\n",
        "\n",
        "# display_data_properties(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "miTximE1zPA3"
      },
      "source": [
        "# 2. Transformers section"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdESdns8dckQ"
      },
      "source": [
        "## 2.1. Completing project setup for transformer models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SyPJopidcKWT"
      },
      "outputs": [],
      "source": [
        "working_dir = os.getcwd()   # Sets the working directory for the project\n",
        "project_dataset = prepare_dataset()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class_samples = [9023, 12366, 6416]\n",
        "class_weights = [1 - (x / sum(class_samples)) for x in class_samples]\n",
        "class_weights = torch.FloatTensor(class_weights).to(device)\n",
        "\n",
        "model_checkpoint = \"distilbert-base-uncased\"\n",
        "\n",
        "if not os.path.exists(working_dir + \"/output-\" + model_checkpoint):\n",
        "  os.makedirs(working_dir + \"/output-\" + model_checkpoint)\n",
        "model_output_dir = working_dir + \"/output-\" + model_checkpoint\n",
        "\n",
        "if not os.path.exists(working_dir + \"/best_models/\" + model_checkpoint):\n",
        "  os.makedirs(working_dir + \"/best_models/\" + model_checkpoint)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33uwRiHu0IlS"
      },
      "source": [
        "## 2.2 Preparing input data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJuFF5nXwzs5"
      },
      "source": [
        "2.2.1. Tokenizating the input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hupzd_EpeRGy"
      },
      "outputs": [],
      "source": [
        "def encode_dataset(tokenizer, project_dataset):\n",
        "  \n",
        "  # Initializing the tokenizer\n",
        "  tokenizer = tokenizer\n",
        "\n",
        "  # Defining tokenization function\n",
        "  def tokenize(batch):\n",
        "    return tokenizer(batch[\"text\"], padding=True, truncation=True)\n",
        "    \n",
        "  # Applying the tokenize function across all the datasets (train/validation/test)\n",
        "  return project_dataset.map(tokenize, batched=True, batch_size=None)\n",
        "\n",
        "dataset_encoded = encode_dataset(tokenizer, project_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UC0-hA3q0PUR"
      },
      "source": [
        "## 2.3 Training a classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3PVEukD0WuQ"
      },
      "source": [
        "### 2.3.1 Training a classifier using feature extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXYWhi1opY1f"
      },
      "source": [
        "2.3.1.1 Extracting the last hidden states"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KOkRKG1FpVoF"
      },
      "outputs": [],
      "source": [
        "def extract_hidden_states_features(model_checkpoint, dataset_encoded):\n",
        "\n",
        "  model = AutoModel.from_pretrained(model_checkpoint).to(device)\n",
        "\n",
        "  # Extracting the last hidden states\n",
        "  def extract_hidden_states(batch):\n",
        "    inputs = {k:v.to(device) for k,v in batch.items() if k in tokenizer.model_input_names}\n",
        "    with torch.no_grad():\n",
        "      last_hidden_state = model(**inputs).last_hidden_state\n",
        "    return {\"hidden_state\": last_hidden_state[:, 0].cpu().numpy()}\n",
        "\n",
        "  # Extracting hidden states across all dataset splits in one go\n",
        "  dataset_encoded.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
        "  dataset_hidden = dataset_encoded.map(extract_hidden_states, batched=True)\n",
        "\n",
        "  return dataset_hidden\n",
        "\n",
        "dataset_hidden = extract_hidden_states_features(model_checkpoint, dataset_encoded)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2514Zq61eia"
      },
      "source": [
        "2.3.1.2 Creating train, validation, test datasets of the hidden states as required for the classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5CLnSniv1jez"
      },
      "outputs": [],
      "source": [
        "# Creating a feature matrix\n",
        "X_train = np.array(dataset_hidden[\"train\"][\"hidden_state\"])\n",
        "X_valid = np.array(dataset_hidden[\"valid\"][\"hidden_state\"])\n",
        "X_test = np.array(dataset_hidden[\"test\"][\"hidden_state\"])\n",
        "y_train = np.array(dataset_hidden[\"train\"][\"label\"])\n",
        "y_valid = np.array(dataset_hidden[\"valid\"][\"label\"])\n",
        "y_test = np.array(dataset_hidden[\"test\"][\"label\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCS6ZHs52Osv"
      },
      "source": [
        "2.3.1.3 Training an MLP classifier using the hidden states and labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U4cwLzs_2Slt"
      },
      "outputs": [],
      "source": [
        "mlp = MLPClassifier(random_state=0,\n",
        "                    validation_fraction=0.2, \n",
        "                    early_stopping=True)\n",
        "mlp.fit(X_train, y_train)\n",
        "print(\"Training accuracy: %.2f %%\" % (mlp.score(X_train, y_train) * 100))\n",
        "print(\"Validation accuracy: %.2f %%\"  % (mlp.score(X_valid, y_valid) * 100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r4znm8gWz0uL"
      },
      "outputs": [],
      "source": [
        "def hyperparameter_search_mlp_model():\n",
        "  PARAM_GRID = {\n",
        "     'learning_rate_init': [0.001, 0.0001, 0.00001],\n",
        "     'hidden_layer_sizes': [(512, 512, 512, 512, 512), (1024, 1024, 1024, 1024)],\n",
        "     'alpha': [0.0001, 0.001]\n",
        "     }\n",
        "\n",
        "  mlp = MLPClassifier(random_state=0,\n",
        "                      validation_fraction=0.2, \n",
        "                      early_stopping=True)\n",
        "\n",
        "  gs_clf = GridSearchCV(mlp, PARAM_GRID, verbose=3).fit(X_train, y_train)\n",
        "  mlp = gs_clf.best_estimator_\n",
        "  print(\"Best model training accuracy: %.2f %%\" % (mlp.score(X_train, y_train) * 100))\n",
        "  print(\"Best model validation accuracy: %.2f %%\"  % (mlp.score(X_valid, y_valid) * 100))\n",
        "\n",
        "# hyperparameter_search_mlp_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7D9P7hzRMs43"
      },
      "source": [
        "2.3.1.4 Generating results to document a baseline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQBLGRe2N4Bq"
      },
      "source": [
        "Plots the training loss and validation accuracy of the MLP model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MnH88pr27DU2"
      },
      "outputs": [],
      "source": [
        "plt.plot(mlp.loss_curve_, label=\"training loss\")\n",
        "plt.plot(mlp.validation_scores_, label=\"validation accuracy\")\n",
        "plt.legend();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmVL8NgFXZ8X"
      },
      "source": [
        "Provides the confusion matrix and classification report for validation data and test data of the project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDhp-lRz7Z0a"
      },
      "outputs": [],
      "source": [
        "y_preds_v = mlp.predict(X_valid)\n",
        "print_classification_report(y_preds_v, y_valid, \"project_dataset[\\\"valid\\\"] using MLP\")\n",
        "\n",
        "y_preds_t = mlp.predict(X_test)\n",
        "print_classification_report(y_preds_t, y_test, \"project_dataset[\\\"test\\\"] using MLP\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pq53_0oIY8QN"
      },
      "source": [
        "2.3.1.5 Saving the trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K17frxcOZHwJ"
      },
      "outputs": [],
      "source": [
        "mlp_save_filename = working_dir + \"/best_models/mlp.joblib\"\n",
        "dump(mlp, mlp_save_filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NY_eY6OHxVGW"
      },
      "source": [
        "2.3.1.5 Loading saved model and testing performance on test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JD4_zH6mxdx2"
      },
      "outputs": [],
      "source": [
        "loaded_mlp = load(working_dir + \"/best_models/mlp.joblib\")\n",
        "y_preds_t = loaded_mlp.predict(X_test)\n",
        "print_classification_report(y_preds_t, y_test, \"project_dataset[\\\"test\\\"] using MLP\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdd_BL_Key8Y"
      },
      "source": [
        "### 2.3.2 Training a classifier using fine tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBmZJaUXaKbo"
      },
      "source": [
        "2.3.2.1 Defining functions required for training a classifier by fine-tuning a transformer model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lM41LGHgAJSd"
      },
      "outputs": [],
      "source": [
        "# Defining the performance metrics that will be used to evaluate our model's performance during fine-tuning\n",
        "def compute_metrics(pred):\n",
        "  labels = pred.label_ids\n",
        "  preds = pred.predictions.argmax(-1)\n",
        "  loss_fct = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
        "  loss_val = loss_fct(torch.from_numpy(pred.predictions).to(device), torch.from_numpy(labels).to(device))\n",
        "  acc = accuracy_score(labels, preds)\n",
        "  f1 = f1_score(labels, preds, average=\"weighted\")\n",
        "  prec = precision_score(labels, preds, average=\"weighted\")\n",
        "  rec = recall_score(labels, preds, average=\"weighted\")\n",
        "  return {\"accuracy\": acc, \"precision\": prec, \"recall\": rec, \"f1_score\": f1, \"loss\": loss_val}\n",
        "\n",
        "\n",
        "def model_init():\n",
        "  dropout_val = 0.3\n",
        "  try:\n",
        "    with open(model_output_dir + \"/model_hp_values.json\", \"r\") as f:\n",
        "      prev_hp = json.load(f)\n",
        "      dropout_val = prev_hp[\"dropout\"]\n",
        "      print(\"Found previously searched dropout value: \", dropout_val)\n",
        "  except:\n",
        "    print(\"No previous dropout value found. Using default setting of\", dropout_val)\n",
        "\n",
        "  if model_checkpoint == \"distilbert-base-uncased\":\n",
        "    return (AutoModelForSequenceClassification.from_pretrained(model_checkpoint, \n",
        "                                                             num_labels=3, \n",
        "                                                             return_dict=True, \n",
        "                                                             dropout=dropout_val,\n",
        "                                                             attention_dropout=dropout_val,\n",
        "                                                             qa_dropout=dropout_val).to(device))\n",
        "  elif model_checkpoint == \"bert-base-uncased\":\n",
        "    return (AutoModelForSequenceClassification.from_pretrained(model_checkpoint, \n",
        "                                                             num_labels=3, \n",
        "                                                             return_dict=True, \n",
        "                                                             attention_probs_dropout_prob=dropout_val,\n",
        "                                                             hidden_dropout_prob=dropout_val,\n",
        "                                                             classifier_dropout=dropout_val).to(device))\n",
        "\n",
        "def compute_objective(metrics):\n",
        "  return metrics[\"eval_loss\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-A7-uO3ZaoWQ"
      },
      "source": [
        "2.3.2.2 Defining the project trainer class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7vQUnBLrr4AS"
      },
      "outputs": [],
      "source": [
        "class ProjectTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        labels = inputs.get(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.get(\"logits\")\n",
        "        loss_fct = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
        "        loss = loss_fct(logits, labels)\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "training_args = TrainingArguments(output_dir=model_output_dir,\n",
        "                                  evaluation_strategy=\"steps\",\n",
        "                                  logging_strategy=\"steps\",\n",
        "                                  eval_steps=500,\n",
        "                                  disable_tqdm=False,\n",
        "                                  logging_dir=model_output_dir + \"/logs\",\n",
        "                                  save_total_limit=1,\n",
        "                                  load_best_model_at_end=True)\n",
        "\n",
        "trainer = ProjectTrainer(args=training_args,\n",
        "                            tokenizer=tokenizer,\n",
        "                            train_dataset=dataset_encoded[\"train\"],\n",
        "                            eval_dataset=dataset_encoded[\"valid\"],\n",
        "                            model_init=model_init,\n",
        "                            compute_metrics=compute_metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpA8EFPAht0C"
      },
      "source": [
        "2.3.2.3 Performing hyperparameter tuning for the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yo0GLfdhBcul"
      },
      "outputs": [],
      "source": [
        "def project_hyperparameter_tuning():\n",
        "\n",
        "  hp_search_space = dict()\n",
        "  try:\n",
        "    with open(model_output_dir + \"/model_hp_values.json\", \"r\") as f:\n",
        "      prev_hp = json.load(f)\n",
        "    plr = prev_hp[\"learning_rate\"]\n",
        "\n",
        "    hp_search_space= {\n",
        "        \"per_device_train_batch_size\": tune.choice([8, 16, 32, 64]),\n",
        "        \"learning_rate\": tune.loguniform(plr * 0.1, plr * 10),\n",
        "        \"weight_decay\": tune.choice([0.1, 0.2, 0.3]),\n",
        "        \"num_train_epochs\": tune.choice([1, 2, 3, 4, 5])\n",
        "    }\n",
        "  except:\n",
        "    hp_search_space = {\n",
        "        \"per_device_train_batch_size\": tune.choice([8, 16, 32, 64]),\n",
        "        \"learning_rate\": tune.loguniform(1e-5, 1e-4),\n",
        "        \"weight_decay\": tune.choice([0.1, 0.2, 0.3]),\n",
        "        \"num_train_epochs\": tune.choice([1, 2, 3, 4, 5])\n",
        "    }\n",
        "\n",
        "  if model_checkpoint == \"distilbert-base-uncased\":\n",
        "    hp_search_space[\"dropout\"] = tune.uniform(0.1, 0.3)\n",
        "    hp_search_space[\"attention_dropout\"] = tune.uniform(0.1, 0.3)\n",
        "    hp_search_space[\"qa_dropout\"] = tune.uniform(0.1, 0.3)\n",
        "  elif model_checkpoint == \"bert-base-uncased\":\n",
        "    hp_search_space[\"attention_probs_dropout_prob\"] = tune.uniform(0.1, 0.3)\n",
        "    hp_search_space[\"hidden_dropout_prob\"] = tune.uniform(0.1, 0.3)\n",
        "    hp_search_space[\"classifier_dropout\"] = tune.uniform(0.1, 0.3)\n",
        "\n",
        "  best_trial = None\n",
        "  try:\n",
        "    best_trial = trainer.hyperparameter_search(hp_space=lambda _: hp_search_space,\n",
        "                                             compute_objective=compute_objective,\n",
        "                              direction=\"minimize\",\n",
        "                              backend=\"ray\",\n",
        "                              n_trials=50,\n",
        "                              search_alg=HyperOptSearch(metric=\"objective\", mode=\"min\"),\n",
        "                              scheduler=ASHAScheduler(metric=\"objective\", mode=\"min\"),\n",
        "                              local_dir=model_output_dir + \"/ray_asha_results_bert\",\n",
        "                              log_to_file=True)\n",
        "\n",
        "    save_best_model = False\n",
        "    if \"objective\" not in hp_search_space:\n",
        "      save_best_model = True\n",
        "    if (\"objective\" in hp_search_space) and (hp_search_space[\"objective\"] > best_trial.objective):\n",
        "      print(\"New best hyperparameters found.\")\n",
        "      save_best_model = True\n",
        "\n",
        "    if save_best_model == True:\n",
        "      save_model_dir = working_dir + \"/best_models/\" + model_checkpoint\n",
        "      for f in os.listdir(save_model_dir):\n",
        "          os.remove(os.path.join(save_model_dir, f))\n",
        "\n",
        "      best_run_dir = model_output_dir + \"/run-\" + best_trial.run_id\n",
        "      best_model_checkpoint = sorted(os.listdir(best_run_dir))[-1]\n",
        "      copy_model_dir = os.path.join(best_run_dir, best_model_checkpoint)\n",
        "\n",
        "      for f in os.listdir(copy_model_dir):\n",
        "        shutil.copy(os.path.join(copy_model_dir, f), save_model_dir)\n",
        "      shutil.make_archive(save_model_dir, 'zip', save_model_dir)\n",
        "\n",
        "      print(\"Saved model associated with the new best hyperparameters in this directory: \", save_model_dir)\n",
        "      print(\"Download the zip file to use the model later.\")\n",
        "    else:\n",
        "      print(\"Could not find better hyperparameters in this search.\")\n",
        "\n",
        "    for f in os.listdir(model_output_dir):\n",
        "      path = os.path.join(model_output_dir, f)\n",
        "      try:\n",
        "        shutil.rmtree(path)\n",
        "      except OSError:\n",
        "        os.remove(path)\n",
        "    print(\"Cleared all files from this hyperparameter search from the following directory: \", model_output_dir)\n",
        "\n",
        "    hp_values = best_trial.hyperparameters\n",
        "    hp_values[\"objective\"] = best_trial.objective\n",
        "    hp_values_json = json.dumps(hp_values)\n",
        "    with open(model_output_dir + \"/model_hp_values.json\", \"w\") as f:\n",
        "      f.write(hp_values_json)\n",
        "    print(\"New hyperparameters written to \" + model_output_dir + \"/model_hp_values.json\")\n",
        "  except:\n",
        "    print(\"Hyperparameter search did not complete successfully.\")\n",
        "\n",
        "  return best_trial\n",
        "\n",
        "current_best_trial = project_hyperparameter_tuning()\n",
        "print(\"Best hyperparameters from this search: \", current_best_trial)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zslzR8fhKS0l"
      },
      "outputs": [],
      "source": [
        "save_model_dir = working_dir + \"/best_models/\" + model_checkpoint\n",
        "shutil.make_archive(save_model_dir, 'zip', save_model_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDJtyd80h8WR"
      },
      "source": [
        "2.3.2.4 Training the model with the last best set of hyperparameters found.\n",
        "\n",
        "This function is written because it was observed that sometimes the hyperparameter tuning phase does not complete successfully due to memory related issues. This function allows us work with any improved hyperparameters found during unsuccessful hyperparameter tuning attempts by manually going through the logs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bg-fTiuDXbZQ"
      },
      "outputs": [],
      "source": [
        "def train_with_last_best_hyperparameters():\n",
        "  try:\n",
        "    with open(model_output_dir + \"/model_hp_values.json\", \"r\") as f:\n",
        "      hp_values = json.load(f)\n",
        "    print(\"Set trainer.args using previous hyperparamter search results.\")\n",
        "  except:\n",
        "    hp_values = dict()\n",
        "    hp_values[\"learning_rate\"] = 1.5544610095816334e-05\n",
        "    hp_values[\"num_train_epochs\"] = 2\n",
        "    hp_values[\"per_device_train_batch_size\"] = 64\n",
        "    hp_values[\"seed\"] = 25\n",
        "    hp_values[\"weight_decay\"] = 0.1\n",
        "    hp_values[\"dropout\"] = 0.12078133498900412\n",
        "    print(\"No previous hyperparameter search results found. Manually setting the hyperparameter values.\")\n",
        "\n",
        "  for n, v in hp_values.items():\n",
        "    if n == \"seed\":\n",
        "      v = int(v)\n",
        "    setattr(trainer.args, n, v)\n",
        "  setattr(trainer.args, \"callbacks\", [EarlyStoppingCallback(early_stopping_patience=3)])\n",
        "\n",
        "  trainer.train()\n",
        "\n",
        "  model_save_path = model_output_dir +  \"/best_model\"\n",
        "  tokenizer.save_pretrained(model_save_path)\n",
        "  trainer.save_model(model_save_path)\n",
        "\n",
        "  train_losses = list()\n",
        "  validation_losses = list()\n",
        "\n",
        "  for entry in trainer.state.log_history:\n",
        "    if \"loss\" in entry:\n",
        "      train_losses.append(entry[\"loss\"])\n",
        "    if \"eval_loss\" in entry:\n",
        "      validation_losses.append(entry[\"eval_loss\"])\n",
        "\n",
        "  plt.plot(train_losses, color=\"blue\")\n",
        "  plt.plot(validation_losses, color=\"red\")\n",
        "  plt.ylim(-0.05, 2)\n",
        "  plt.xlabel(\"Steps\")\n",
        "  plt.ylabel(\"loss\")\n",
        "  plt.legend([\"train_loss\", \"validation_loss\"])\n",
        "  plt.title(\"Loss every 500 steps\")\n",
        "  plt.show()\n",
        "\n",
        "train_with_last_best_hyperparameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0Da-x_akrzw"
      },
      "source": [
        "2.3.2.5 Generating results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFtbalJVlhgN"
      },
      "source": [
        "Provides the confusion matrix and classification report for validation data and test data of the project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4or4NWTNhjM9"
      },
      "outputs": [],
      "source": [
        "y_valid = np.array(dataset_encoded[\"valid\"][\"label\"])\n",
        "validation_predictions_all = trainer.predict(dataset_encoded[\"valid\"])\n",
        "y_preds_v = np.argmax(validation_predictions_all.predictions, axis=1)\n",
        "print_classification_report(y_preds_v, y_valid, \"project_dataset[\\\"valid\\\"] using finetuned \" + model_checkpoint)\n",
        "\n",
        "y_test = np.array(dataset_encoded[\"test\"][\"label\"])\n",
        "test_predictions_all = trainer.predict(dataset_encoded[\"test\"])\n",
        "y_preds_t = np.argmax(test_predictions_all.predictions, axis=1)\n",
        "print_classification_report(y_preds_t, y_test, \"project_dataset[\\\"test\\\"] using finetuned \" + model_checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLJP0tUqpst-"
      },
      "source": [
        "2.3.2.5 Loading saved model and testing performance on test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fr0kN99FpLtU"
      },
      "outputs": [],
      "source": [
        "def evaluate_test_data_using_saved_model(dataset):\n",
        "  # Load model\n",
        "  try:\n",
        "    model_load_path = working_dir +  \"/best_models/\" + model_checkpoint \n",
        "    loaded_tokenizer = AutoTokenizer.from_pretrained(model_load_path)\n",
        "    loaded_m = (AutoModelForSequenceClassification.from_pretrained(model_load_path, num_labels=3).to(device))\n",
        "    trainer = ProjectTrainer(model=loaded_m)\n",
        "\n",
        "    dataset_encoded = encode_dataset(loaded_tokenizer, dataset)\n",
        "    y_test = np.array(dataset_encoded[\"label\"])\n",
        "    labels = dataset.features[\"label\"].names\n",
        "\n",
        "    test_predictions_all = trainer.predict(dataset_encoded)\n",
        "    y_preds_t = np.argmax(test_predictions_all.predictions, axis=1)\n",
        "    print_classification_report(y_preds_t, y_test, \"project_dataset[\\\"test\\\"] using finetuned \" + model_checkpoint)\n",
        "  except FileNotFoundError:\n",
        "    print(\"Could not load a model from the provided path.\")\n",
        "  except Exception as e:\n",
        "    print(\"Could not complete the opration because of the following error: \", e)\n",
        "evaluate_test_data_using_saved_model(project_dataset[\"test\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "POntTCqot1fd"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "W22_COMP6321_Project.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}