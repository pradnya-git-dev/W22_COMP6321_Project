{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#1. Project Setup"
      ],
      "metadata": {
        "id": "lPfAuEWFp2jZ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nz5gX7XkdGo8"
      },
      "source": [
        "1.1 Installing required modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0PWTVK1iaXKL"
      },
      "outputs": [],
      "source": [
        "# Installs required modules\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IheQ3gZTdQET"
      },
      "source": [
        "1.2. Importing required modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Js-DLnUXa_RA"
      },
      "outputs": [],
      "source": [
        "# Imports required modules\n",
        "import torch\n",
        "import torchtext\n",
        "from datasets import load_dataset, DatasetDict, Features, Value, ClassLabel\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback\n",
        "from transformers import set_seed\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, ConfusionMatrixDisplay, confusion_matrix, classification_report\n",
        "from ray.tune.suggest.hyperopt import HyperOptSearch\n",
        "from ray.tune.schedulers import ASHAScheduler\n",
        "from ray import tune\n",
        "import json\n",
        "import os\n",
        "import shutil\n",
        "from joblib import dump, load\n",
        "import re \n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# Sets seeds to zero\n",
        "set_seed(0)\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeqViK9HdT0o"
      },
      "source": [
        "1.3. Defining utility functions for the entire project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NlmT51ZJcCf_"
      },
      "outputs": [],
      "source": [
        "def prepare_dataset(working_dir=os.getcwd()):\n",
        "  \"\"\"This function prepares and returns the dataset for the project.\n",
        "\n",
        "  Arguments\n",
        "  ---------\n",
        "  working_dir : str\n",
        "    A string value representing the current working directory \n",
        "  \n",
        "  Returns\n",
        "  ---------\n",
        "  project_dataset : datasets.dataset_dict.DatasetDict\n",
        "    A Dataset dictionary object containing the train, validation, and test data\n",
        "  \"\"\"\n",
        "\n",
        "  train_file = working_dir + \"/train_dataset_processed.csv\"\n",
        "  test_file = working_dir + \"/test_dataset_processed.csv\"\n",
        "\n",
        "  data_files = {\"train\": train_file, \"test\": test_file}\n",
        "  class_names = [\"negative\", \"neutral\", \"positive\"]\n",
        "  data_columns = Features({\"label\": ClassLabel(names=class_names), \"text\": Value(\"string\")})\n",
        "  original_data = load_dataset(\"csv\", data_files=data_files, column_names=[\"label\", \"text\"], features=data_columns)\n",
        "\n",
        "  train_valid_data = original_data[\"train\"].train_test_split(test_size=0.2)\n",
        "  train_dataset = train_valid_data[\"train\"]\n",
        "  valid_dataset = train_valid_data[\"test\"]\n",
        "  project_dataset = DatasetDict({\n",
        "      \"train\": train_dataset,\n",
        "      \"valid\": valid_dataset,\n",
        "      \"test\": original_data[\"test\"]})\n",
        "\n",
        "  # Removing empty text entries\n",
        "  project_dataset = project_dataset.filter(lambda example: example[\"text\"] != None)\n",
        "  return project_dataset\n",
        "\n",
        "\n",
        "def plot_confusion_matrix(y_preds, y_true, labels, title):\n",
        "  \"\"\"This function displays the confusion matrix for provided predictions by a classifier.\n",
        "\n",
        "  Arguments\n",
        "  ---------\n",
        "  y_preds : numpy.ndarray\n",
        "    Predictions by a classifier\n",
        "  y_true : numpy.ndarray\n",
        "    Labels corresponding to the predictions in y_preds\n",
        "  labels : list\n",
        "    Label names to be used for the confusion matrix\n",
        "  title : str\n",
        "    Title to be used for the confusion matrix\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  cm = confusion_matrix(y_true, y_preds, normalize=\"true\")\n",
        "  fix, ax = plt.subplots(figsize=(6, 6))\n",
        "  cm_display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
        "  cm_display.plot(cmap=\"Blues\", values_format=\".2f\", ax=ax, colorbar=False)\n",
        "  plt.title(\"Normalized confusion matrix for \" + title)\n",
        "  plt.show()\n",
        "\n",
        "def print_classification_report(y_preds, y_true, title):\n",
        "  \"\"\"This function displays the classification report for provided predictions by a classifier.\n",
        "\n",
        "  Arguments\n",
        "  ---------\n",
        "  y_preds : numpy.ndarray\n",
        "    Predictions by a classifier\n",
        "  y_true : numpy.ndarray\n",
        "    Labels corresponding to the predictions in y_preds\n",
        "  title : str\n",
        "    Title to be used for the confusion matrix\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  labels = [\"negative\", \"neutral\", \"positive\"]\n",
        "  plot_confusion_matrix(y_preds, y_true, labels, title)\n",
        "  print(\"\\n\")\n",
        "  print(\"Classification report for \" + title)\n",
        "  acc = accuracy_score(y_true, y_preds)\n",
        "  f1 = f1_score(y_true, y_preds, average=\"weighted\")\n",
        "  prec = precision_score(y_true, y_preds, average=\"weighted\")\n",
        "  rec = recall_score(y_true, y_preds, average=\"weighted\")\n",
        "  \n",
        "  print(\"-\" * 75)\n",
        "  print(\"Accuracy: %.2f%%\" % (acc * 100))\n",
        "  print(\"Precision: %.2f%%\" % (prec * 100) )\n",
        "  print(\"Recall: %.2f%%\" % (rec * 100))\n",
        "  print(\"F1 score: %.2f%%\" % (f1 * 100) )\n",
        "  print(\"-\" * 75)\n",
        "  print(\"\\n\\n\")\n",
        "\n",
        "def display_data_properties(dataset):\n",
        "  \"\"\"This function displays data distribution among different classes in the dataset.\n",
        "\n",
        "  Arguments\n",
        "  ---------\n",
        "  dataset : datasets.dataset_dict.DatasetDict\n",
        "    A Dataset dictionary object containing one or more of the following -  train, validation, and test data\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  # Uses pandas to display the data distribution\n",
        "  dataset.set_format(type=\"pandas\")\n",
        "  train_df = dataset[\"train\"][:]\n",
        "  valid_df = dataset[\"valid\"][:]\n",
        "  test_df = dataset[\"test\"][:]\n",
        "\n",
        "  def label_int2str(row):\n",
        "    \"\"\"\n",
        "    This function converts a label's integer value to a string value.\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    row : int\n",
        "      Integer value of the label\n",
        "    \n",
        "    \"\"\"\n",
        "\n",
        "    return dataset[\"train\"].features[\"label\"].int2str(row)\n",
        "\n",
        "  train_df[\"label_name\"] = train_df[\"label\"].apply(label_int2str)\n",
        "  valid_df[\"label_name\"] = valid_df[\"label\"].apply(label_int2str)\n",
        "  test_df[\"label_name\"] = test_df[\"label\"].apply(label_int2str)\n",
        "\n",
        "  print(\"Displaying the first 5 examples of the training dataset: \")\n",
        "  print(\"-\" * 75)\n",
        "  print(train_df.head())\n",
        "  print(\"-\" * 75)\n",
        "\n",
        "  # Displays the frequency of classes in the training data\n",
        "  train_df[\"label_name\"].value_counts(ascending=True).plot.barh()\n",
        "  plt.title(\"Frequency of Classes in training data\")\n",
        "  plt.show()\n",
        "\n",
        "  # Displays the frequency of classes in the validation data\n",
        "  plt.figure()\n",
        "  valid_df[\"label_name\"].value_counts(ascending=True).plot.barh()\n",
        "  plt.title(\"Frequency of Classes in validation data\")\n",
        "  plt.show()\n",
        "\n",
        "  # Displays the frequency of classes in the test data\n",
        "  plt.figure()\n",
        "  test_df[\"label_name\"].value_counts(ascending=True).plot.barh()\n",
        "  plt.title(\"Frequency of Classes in test data\")\n",
        "  plt.show()\n",
        "\n",
        "  # Resetting the dataset format\n",
        "  dataset.reset_format()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "miTximE1zPA3"
      },
      "source": [
        "# 2. Transformers section"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdESdns8dckQ"
      },
      "source": [
        "## 2.1. Completing project setup for transformer models"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below code cell is used to provide the value for \"model_checkpoint\" as one of the following: \"distilbert-base-uncased\" or \"bert-base-uncased\".\n",
        "\n",
        "This sets the pretrained transformer model to be used for the current execution of the model as either BERT or DistilBERT."
      ],
      "metadata": {
        "id": "qhkgqrref9L7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SyPJopidcKWT"
      },
      "outputs": [],
      "source": [
        "model_checkpoint = \"distilbert-base-uncased\"    # Sets the model to be used for the current execution of the project\n",
        "\n",
        "working_dir = os.getcwd()               # Sets the working directory for the project\n",
        "project_dataset = prepare_dataset()     # Prepares project dataset to be used\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")   # Uses GPU if available otherwise uses CPU\n",
        "\n",
        "# Creates the class weights tensor - used for cross entropy calculation\n",
        "class_samples = [9023, 12366, 6416]\n",
        "class_weights = [1 - (x / sum(class_samples)) for x in class_samples]\n",
        "class_weights = torch.FloatTensor(class_weights).to(device)\n",
        "\n",
        "# Creates the output directory for the current model (if one does not exist)\n",
        "if not os.path.exists(working_dir + \"/output-\" + model_checkpoint):\n",
        "  os.makedirs(working_dir + \"/output-\" + model_checkpoint)\n",
        "model_output_dir = working_dir + \"/output-\" + model_checkpoint\n",
        "\n",
        "# Creates a directory to save the best models (if onr does not exist)\n",
        "if not os.path.exists(working_dir + \"/best_models/\" + model_checkpoint):\n",
        "  os.makedirs(working_dir + \"/best_models/\" + model_checkpoint)\n",
        "\n",
        "# Instantiates a tokenizer based on the provided name of the pretrained model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verifying the project setup"
      ],
      "metadata": {
        "id": "jURTsInJe_QX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "assert model_checkpoint == \"distilbert-base-uncased\" or model_checkpoint == \"bert-base-uncased\", \"Expected model_checkpoint to be either \\\"distilbert-base-uncased\\\" or \\\"bert-base-uncased\\\"\"\n",
        "assert isinstance(working_dir, str), \"Expected working_dir to be a string value\"\n",
        "assert working_dir == os.getcwd(), \"Expected working_dir to be the current working dir\"\n",
        "assert isinstance(project_dataset, DatasetDict), \"Expected project_dataset to be a datasets.dataset_dict.DatasetDict\"\n",
        "assert isinstance(class_samples, list), \"Expected class_samples to be a list\"\n",
        "assert isinstance(class_weights, torch.Tensor), \"Expected class_weights to be a torch.Tensor\"\n",
        "assert model_output_dir == working_dir + \"/output-\" + model_checkpoint, \"Unexpected value for model_outpur_dir\"\n",
        "print(\"Project setup is OK.\")"
      ],
      "metadata": {
        "id": "Ci5llkDUaeHx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33uwRiHu0IlS"
      },
      "source": [
        "## 2.2 Preparing input data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJuFF5nXwzs5"
      },
      "source": [
        "2.2.1. Tokenizating the input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hupzd_EpeRGy"
      },
      "outputs": [],
      "source": [
        "def encode_dataset(tokenizer, project_dataset):\n",
        "  \"\"\"This function uses the provided tokenizer to tokenize and encode samples in project_dataset as their numerical vectors.\n",
        "\n",
        "  Arguments\n",
        "  ---------\n",
        "  tokenizer : \n",
        "    The tokenizer instantiated based on the pretrained model being used for the project\n",
        "  project_dataset : datasets.dataset_dict.DatasetDict\n",
        "    A Dataset dictionary object containing one or more of the samples and their labels\n",
        "  \n",
        "  Returns\n",
        "  ---------\n",
        "  dataset_encoded : datasets.dataset_dict.DatasetDict\n",
        "    The encoded dataset containing encoded samples and their labels\n",
        "  \"\"\"\n",
        "  \n",
        "  # Initializes the tokenizer\n",
        "  tokenizer = tokenizer\n",
        "\n",
        "  # Defines the tokenization function\n",
        "  def tokenize(batch):\n",
        "    \"\"\"This function applies tokenizer to a batch of samples\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    batch : datasets.arrow_dataset.Batch\n",
        "      A batch of samples\n",
        "    \"\"\"\n",
        "    return tokenizer(batch[\"text\"], padding=True, truncation=True)\n",
        "    \n",
        "  # Applies the tokenize function to the provided project_dataset\n",
        "  return project_dataset.map(tokenize, batched=True, batch_size=None)\n",
        "\n",
        "dataset_encoded = encode_dataset(tokenizer, project_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verifying encoded dataset"
      ],
      "metadata": {
        "id": "zNjJ_NERZNoC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "assert \"input_ids\" in dataset_encoded[\"train\"].column_names, \"dataset_encoded is not correctly formed\"\n",
        "assert \"attention_mask\" in dataset_encoded[\"train\"].column_names, \"dataset_encoded is not correctly formed\"\n",
        "assert \"text\" in dataset_encoded[\"train\"].column_names, \"dataset_encoded is not correctly formed\"\n",
        "assert \"label\" in dataset_encoded[\"train\"].column_names, \"dataset_encoded is not correctly formed\"\n",
        "print(\"Encoded dataset formed correctly.\")"
      ],
      "metadata": {
        "id": "0uKG6cZHZIPg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UC0-hA3q0PUR"
      },
      "source": [
        "## 2.3 Training a classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3PVEukD0WuQ"
      },
      "source": [
        "### 2.3.1 Training a classifier using feature extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXYWhi1opY1f"
      },
      "source": [
        "2.3.1.1 Extracting the last hidden states"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KOkRKG1FpVoF"
      },
      "outputs": [],
      "source": [
        "def extract_hidden_states_features(model_checkpoint, dataset_encoded, tokenizer):\n",
        "  \"\"\"This function passes the encoded input through a pretrained model and extracts and returns the last hidden states.\n",
        "\n",
        "  Arguments\n",
        "  ---------\n",
        "  model_checkpoint : str\n",
        "    The name of the pretrained model to be used\n",
        "  dataset_encoded : datasets.dataset_dict.DatasetDict\n",
        "    The encoded dataset containing encoded samples and their labels\n",
        "  tokenizer : \n",
        "    The tokenizer instantiated based on the pretrained model being used for the project\n",
        "  \n",
        "  Returns\n",
        "  ---------\n",
        "  dataset_hidden : datasets.dataset_dict.DatasetDict\n",
        "    The dataset containing the last hidden states and labels\n",
        "  \"\"\"\n",
        "\n",
        "  model = AutoModel.from_pretrained(model_checkpoint).to(device)\n",
        "\n",
        "  def extract_hidden_states(batch):\n",
        "    \"\"\"This function extracts the hidden states for a batch of samples\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    batch : dict\n",
        "      A batch of samples\n",
        "    \"\"\"\n",
        "    inputs = {k:v.to(device) for k,v in batch.items() if k in tokenizer.model_input_names}\n",
        "    with torch.no_grad():\n",
        "      last_hidden_state = model(**inputs).last_hidden_state\n",
        "    return {\"hidden_state\": last_hidden_state[:, 0].cpu().numpy()}\n",
        "\n",
        "  # Extracting hidden states for all samples using map\n",
        "  dataset_encoded.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
        "  dataset_hidden = dataset_encoded.map(extract_hidden_states, batched=True)\n",
        "\n",
        "  return dataset_hidden\n",
        "\n",
        "dataset_hidden = extract_hidden_states_features(model_checkpoint, dataset_encoded, tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verifying the extracted hidden states"
      ],
      "metadata": {
        "id": "bRPLP16bdOq2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "assert \"input_ids\" in dataset_hidden[\"train\"].column_names, \"dataset_hidden is not correctly formed\"\n",
        "assert \"attention_mask\" in dataset_hidden[\"train\"].column_names, \"dataset_hidden is not correctly formed\"\n",
        "assert \"text\" in dataset_hidden[\"train\"].column_names, \"dataset_hidden is not correctly formed\"\n",
        "assert \"label\" in dataset_hidden[\"train\"].column_names, \"dataset_hidden is not correctly formed\"\n",
        "assert \"hidden_state\" in dataset_hidden[\"train\"].column_names, \"dataset_hidden is not correctly formed\"\n",
        "print(\"Hidden states dataset formed correctly.\")"
      ],
      "metadata": {
        "id": "Twkh2ymodOaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2514Zq61eia"
      },
      "source": [
        "2.3.1.2 Creating train, validation, test datasets of the hidden states as required for the classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5CLnSniv1jez"
      },
      "outputs": [],
      "source": [
        "# Creating a feature matrix\n",
        "X_train = np.array(dataset_hidden[\"train\"][\"hidden_state\"])\n",
        "X_valid = np.array(dataset_hidden[\"valid\"][\"hidden_state\"])\n",
        "X_test = np.array(dataset_hidden[\"test\"][\"hidden_state\"])\n",
        "y_train = np.array(dataset_hidden[\"train\"][\"label\"])\n",
        "y_valid = np.array(dataset_hidden[\"valid\"][\"label\"])\n",
        "y_test = np.array(dataset_hidden[\"test\"][\"label\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verifying the created feature matrix"
      ],
      "metadata": {
        "id": "ih8UnIWJdxJn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "assert len(X_train) == len(y_train), \"The length of X_train and y_train must be the same\"\n",
        "assert len(X_valid) == len(y_valid), \"The length of X_valid and y_valid must be the same\"\n",
        "assert len(X_test) == len(y_test), \"The length of X_test and y_test must be the same\"\n",
        "print(\"Feature matrix is OK.\")"
      ],
      "metadata": {
        "id": "d2apjQXXd0SY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCS6ZHs52Osv"
      },
      "source": [
        "2.3.1.3 Training an MLP classifier using the hidden states and labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U4cwLzs_2Slt"
      },
      "outputs": [],
      "source": [
        "mlp = MLPClassifier(random_state=0,\n",
        "                    validation_fraction=0.2, \n",
        "                    early_stopping=True)\n",
        "mlp.fit(X_train, y_train)\n",
        "print(\"Training accuracy: %.2f %%\" % (mlp.score(X_train, y_train) * 100))\n",
        "print(\"Validation accuracy: %.2f %%\"  % (mlp.score(X_valid, y_valid) * 100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7D9P7hzRMs43"
      },
      "source": [
        "2.3.1.4 Generating results to document a baseline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQBLGRe2N4Bq"
      },
      "source": [
        "Plots the training loss and validation accuracy of the MLP model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MnH88pr27DU2"
      },
      "outputs": [],
      "source": [
        "plt.plot(mlp.loss_curve_, label=\"training loss\")\n",
        "plt.plot(mlp.validation_scores_, label=\"validation accuracy\")\n",
        "plt.legend();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmVL8NgFXZ8X"
      },
      "source": [
        "Provides the confusion matrix and classification report for validation data and test data of the project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDhp-lRz7Z0a"
      },
      "outputs": [],
      "source": [
        "y_preds_v = mlp.predict(X_valid)\n",
        "print_classification_report(y_preds_v, y_valid, \"project_dataset[\\\"valid\\\"] using MLP\")\n",
        "\n",
        "y_preds_t = mlp.predict(X_test)\n",
        "print_classification_report(y_preds_t, y_test, \"project_dataset[\\\"test\\\"] using MLP\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pq53_0oIY8QN"
      },
      "source": [
        "2.3.1.5 Saving the trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K17frxcOZHwJ"
      },
      "outputs": [],
      "source": [
        "mlp_save_filename = working_dir + \"/best_models/mlp.joblib\"\n",
        "dump(mlp, mlp_save_filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NY_eY6OHxVGW"
      },
      "source": [
        "2.3.1.5 Loading saved model and testing performance on test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JD4_zH6mxdx2"
      },
      "outputs": [],
      "source": [
        "loaded_mlp = load(working_dir + \"/best_models/mlp.joblib\")\n",
        "y_preds_t = loaded_mlp.predict(X_test)\n",
        "print_classification_report(y_preds_t, y_test, \"DistilBERT + MLP\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdd_BL_Key8Y"
      },
      "source": [
        "### 2.3.2 Training a classifier using fine tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBmZJaUXaKbo"
      },
      "source": [
        "2.3.2.1 Defining functions required for training a classifier by fine-tuning a transformer model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lM41LGHgAJSd"
      },
      "outputs": [],
      "source": [
        "# Defining the performance metrics that will be used to evaluate our model's performance during fine-tuning\n",
        "def compute_metrics(pred):\n",
        "  \"\"\"This function computes and returns the accuracy, precision, recall, F1 score, and categorical cross entropy loss metrics for the provided predictions.\n",
        "\n",
        "  Arguments\n",
        "  ---------\n",
        "  pred : \n",
        "    Object containing the predictions and labels for which the metrics are to be calculated\n",
        "  \"\"\"\n",
        "\n",
        "  print(\"Tpye of pred: \", type(pred))\n",
        "  labels = pred.label_ids\n",
        "  preds = pred.predictions.argmax(-1)\n",
        "  loss_fct = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
        "  loss_val = loss_fct(torch.from_numpy(pred.predictions).to(device), torch.from_numpy(labels).to(device))\n",
        "  acc = accuracy_score(labels, preds)\n",
        "  f1 = f1_score(labels, preds, average=\"weighted\")\n",
        "  prec = precision_score(labels, preds, average=\"weighted\")\n",
        "  rec = recall_score(labels, preds, average=\"weighted\")\n",
        "  return {\"accuracy\": acc, \"precision\": prec, \"recall\": rec, \"f1_score\": f1, \"loss\": loss_val}\n",
        "\n",
        "\n",
        "def model_init():\n",
        "  \"\"\"This function initializes and returns a transformer model based on the current value of \"model_checkpoint\".\n",
        "  \"\"\"\n",
        "  dropout_val = 0.3\n",
        "  try:\n",
        "    with open(model_output_dir + \"/model_hp_values.json\", \"r\") as f:\n",
        "      prev_hp = json.load(f)\n",
        "      dropout_val = prev_hp[\"dropout\"]\n",
        "      print(\"Found previously searched dropout value: \", dropout_val)\n",
        "  except:\n",
        "    print(\"No previous dropout value found. Using default setting of\", dropout_val)\n",
        "\n",
        "  if model_checkpoint == \"distilbert-base-uncased\":\n",
        "    return (AutoModelForSequenceClassification.from_pretrained(model_checkpoint, \n",
        "                                                             num_labels=3, \n",
        "                                                             return_dict=True, \n",
        "                                                             dropout=dropout_val,\n",
        "                                                             attention_dropout=dropout_val,\n",
        "                                                             qa_dropout=dropout_val).to(device))\n",
        "  elif model_checkpoint == \"bert-base-uncased\":\n",
        "    return (AutoModelForSequenceClassification.from_pretrained(model_checkpoint, \n",
        "                                                             num_labels=3, \n",
        "                                                             return_dict=True, \n",
        "                                                             attention_probs_dropout_prob=dropout_val,\n",
        "                                                             hidden_dropout_prob=dropout_val,\n",
        "                                                             classifier_dropout=dropout_val).to(device))\n",
        "\n",
        "def compute_objective(metrics):\n",
        "\n",
        "  return metrics[\"eval_loss\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-A7-uO3ZaoWQ"
      },
      "source": [
        "2.3.2.2 Defining the project trainer class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7vQUnBLrr4AS"
      },
      "outputs": [],
      "source": [
        "class ProjectTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        labels = inputs.get(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.get(\"logits\")\n",
        "        loss_fct = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
        "        loss = loss_fct(logits, labels)\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "training_args = TrainingArguments(output_dir=model_output_dir,\n",
        "                                  evaluation_strategy=\"steps\",\n",
        "                                  logging_strategy=\"steps\",\n",
        "                                  eval_steps=500,\n",
        "                                  disable_tqdm=False,\n",
        "                                  logging_dir=model_output_dir + \"/logs\",\n",
        "                                  save_total_limit=1,\n",
        "                                  load_best_model_at_end=True)\n",
        "\n",
        "trainer = ProjectTrainer(args=training_args,\n",
        "                            tokenizer=tokenizer,\n",
        "                            train_dataset=dataset_encoded[\"train\"],\n",
        "                            eval_dataset=dataset_encoded[\"valid\"],\n",
        "                            model_init=model_init,\n",
        "                            compute_metrics=compute_metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpA8EFPAht0C"
      },
      "source": [
        "2.3.2.3 Performing hyperparameter tuning for the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yo0GLfdhBcul"
      },
      "outputs": [],
      "source": [
        "def project_hyperparameter_tuning(num_trials=20):\n",
        "\n",
        "  hp_search_space = dict()\n",
        "  try:\n",
        "    with open(model_output_dir + \"/model_hp_values.json\", \"r\") as f:\n",
        "      prev_hp = json.load(f)\n",
        "    plr = prev_hp[\"learning_rate\"]\n",
        "\n",
        "    hp_search_space= {\n",
        "        \"per_device_train_batch_size\": tune.choice([8, 16, 32, 64]),\n",
        "        \"learning_rate\": tune.loguniform(plr * 0.1, plr * 10),\n",
        "        \"weight_decay\": tune.choice([0.1, 0.2, 0.3]),\n",
        "        \"num_train_epochs\": tune.choice([1, 2, 3, 4, 5])\n",
        "    }\n",
        "  except:\n",
        "    hp_search_space = {\n",
        "        \"per_device_train_batch_size\": tune.choice([8, 16, 32, 64]),\n",
        "        \"learning_rate\": tune.loguniform(1e-5, 1e-4),\n",
        "        \"weight_decay\": tune.choice([0.1, 0.2, 0.3]),\n",
        "        \"num_train_epochs\": tune.choice([1, 2, 3, 4, 5])\n",
        "    }\n",
        "\n",
        "  if model_checkpoint == \"distilbert-base-uncased\":\n",
        "    hp_search_space[\"dropout\"] = tune.uniform(0.1, 0.3)\n",
        "    hp_search_space[\"attention_dropout\"] = tune.uniform(0.1, 0.3)\n",
        "    hp_search_space[\"qa_dropout\"] = tune.uniform(0.1, 0.3)\n",
        "  elif model_checkpoint == \"bert-base-uncased\":\n",
        "    hp_search_space[\"attention_probs_dropout_prob\"] = tune.uniform(0.1, 0.3)\n",
        "    hp_search_space[\"hidden_dropout_prob\"] = tune.uniform(0.1, 0.3)\n",
        "    hp_search_space[\"classifier_dropout\"] = tune.uniform(0.1, 0.3)\n",
        "\n",
        "  best_trial = None\n",
        "  try:\n",
        "    best_trial = trainer.hyperparameter_search(hp_space=lambda _: hp_search_space,\n",
        "                                             compute_objective=compute_objective,\n",
        "                              direction=\"minimize\",\n",
        "                              backend=\"ray\",\n",
        "                              n_trials=num_trials,\n",
        "                              search_alg=HyperOptSearch(metric=\"objective\", mode=\"min\"),\n",
        "                              scheduler=ASHAScheduler(metric=\"objective\", mode=\"min\"),\n",
        "                              local_dir=model_output_dir + \"/ray_asha_results_bert\",\n",
        "                              log_to_file=True)\n",
        "\n",
        "    save_best_model = False\n",
        "    if \"objective\" not in hp_search_space:\n",
        "      save_best_model = True\n",
        "    if (\"objective\" in hp_search_space) and (hp_search_space[\"objective\"] > best_trial.objective):\n",
        "      print(\"New best hyperparameters found.\")\n",
        "      save_best_model = True\n",
        "\n",
        "    if save_best_model == True:\n",
        "      save_model_dir = working_dir + \"/best_models/\" + model_checkpoint\n",
        "      for f in os.listdir(save_model_dir):\n",
        "          os.remove(os.path.join(save_model_dir, f))\n",
        "\n",
        "      best_run_dir = model_output_dir + \"/run-\" + best_trial.run_id\n",
        "      best_model_checkpoint = sorted(os.listdir(best_run_dir))[-1]\n",
        "      copy_model_dir = os.path.join(best_run_dir, best_model_checkpoint)\n",
        "\n",
        "      for f in os.listdir(copy_model_dir):\n",
        "        shutil.copy(os.path.join(copy_model_dir, f), save_model_dir)\n",
        "      shutil.make_archive(save_model_dir, 'zip', save_model_dir)\n",
        "\n",
        "      print(\"Saved model associated with the new best hyperparameters in this directory: \", save_model_dir)\n",
        "      print(\"Download the zip file to use the model later.\")\n",
        "    else:\n",
        "      print(\"Could not find better hyperparameters in this search.\")\n",
        "\n",
        "    for f in os.listdir(model_output_dir):\n",
        "      path = os.path.join(model_output_dir, f)\n",
        "      try:\n",
        "        shutil.rmtree(path)\n",
        "      except OSError:\n",
        "        os.remove(path)\n",
        "    print(\"Cleared all files from this hyperparameter search from the following directory: \", model_output_dir)\n",
        "\n",
        "    hp_values = best_trial.hyperparameters\n",
        "    hp_values[\"objective\"] = best_trial.objective\n",
        "    hp_values_json = json.dumps(hp_values)\n",
        "    with open(model_output_dir + \"/model_hp_values.json\", \"w\") as f:\n",
        "      f.write(hp_values_json)\n",
        "    print(\"New hyperparameters written to \" + model_output_dir + \"/model_hp_values.json\")\n",
        "  except:\n",
        "    print(\"Hyperparameter search did not complete successfully.\")\n",
        "\n",
        "  return best_trial\n",
        "\n",
        "current_best_trial = project_hyperparameter_tuning()\n",
        "print(\"Best hyperparameters from this search: \", current_best_trial)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDJtyd80h8WR"
      },
      "source": [
        "2.3.2.4 Training the model with the last best set of hyperparameters found.\n",
        "\n",
        "This function is written because it was observed that sometimes the hyperparameter tuning phase does not complete successfully due to memory related issues. This function allows us work with any improved hyperparameters found during unsuccessful hyperparameter tuning attempts by manually going through the logs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bg-fTiuDXbZQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "outputId": "f8281ba7-770d-43d6-ff05-cf8a8c173b0d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='341' max='696' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [341/696 06:57 < 07:17, 0.81 it/s, Epoch 0.98/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "def train_with_last_best_hyperparameters():\n",
        "  try:\n",
        "    with open(model_output_dir + \"/model_hp_values.json\", \"r\") as f:\n",
        "      hp_values = json.load(f)\n",
        "    print(\"Set trainer.args using previous hyperparamter search results.\")\n",
        "  except:\n",
        "    hp_values = dict()\n",
        "    hp_values[\"learning_rate\"] = 1.5544610095816334e-05\n",
        "    hp_values[\"num_train_epochs\"] = 2\n",
        "    hp_values[\"per_device_train_batch_size\"] = 64\n",
        "    hp_values[\"seed\"] = 25\n",
        "    hp_values[\"weight_decay\"] = 0.1\n",
        "    hp_values[\"dropout\"] = 0.12078133498900412\n",
        "    print(\"No previous hyperparameter search results found. Manually setting the hyperparameter values.\")\n",
        "\n",
        "  for n, v in hp_values.items():\n",
        "    if n == \"seed\":\n",
        "      v = int(v)\n",
        "    setattr(trainer.args, n, v)\n",
        "  setattr(trainer.args, \"callbacks\", [EarlyStoppingCallback(early_stopping_patience=3)])\n",
        "\n",
        "  trainer.train()\n",
        "\n",
        "  model_save_path = model_output_dir +  \"/best_model\"\n",
        "  tokenizer.save_pretrained(model_save_path)\n",
        "  trainer.save_model(model_save_path)\n",
        "\n",
        "  train_losses = list()\n",
        "  validation_losses = list()\n",
        "\n",
        "  for entry in trainer.state.log_history:\n",
        "    if \"loss\" in entry:\n",
        "      train_losses.append(entry[\"loss\"])\n",
        "    if \"eval_loss\" in entry:\n",
        "      validation_losses.append(entry[\"eval_loss\"])\n",
        "\n",
        "  plt.plot(train_losses, color=\"blue\")\n",
        "  plt.plot(validation_losses, color=\"red\")\n",
        "  plt.ylim(-0.05, 2)\n",
        "  plt.xlabel(\"Steps\")\n",
        "  plt.ylabel(\"loss\")\n",
        "  plt.legend([\"train_loss\", \"validation_loss\"])\n",
        "  plt.title(\"Loss every 500 steps\")\n",
        "  plt.show()\n",
        "\n",
        "train_with_last_best_hyperparameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0Da-x_akrzw"
      },
      "source": [
        "2.3.2.5 Generating results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFtbalJVlhgN"
      },
      "source": [
        "Provides the confusion matrix and classification report for validation data and test data of the project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4or4NWTNhjM9"
      },
      "outputs": [],
      "source": [
        "y_valid = np.array(dataset_encoded[\"valid\"][\"label\"])\n",
        "validation_predictions_all = trainer.predict(dataset_encoded[\"valid\"])\n",
        "y_preds_v = np.argmax(validation_predictions_all.predictions, axis=1)\n",
        "print_classification_report(y_preds_v, y_valid, \"project_dataset[\\\"valid\\\"] using finetuned \" + model_checkpoint)\n",
        "\n",
        "y_test = np.array(dataset_encoded[\"test\"][\"label\"])\n",
        "test_predictions_all = trainer.predict(dataset_encoded[\"test\"])\n",
        "y_preds_t = np.argmax(test_predictions_all.predictions, axis=1)\n",
        "print_classification_report(y_preds_t, y_test, \"project_dataset[\\\"test\\\"] using finetuned \" + model_checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLJP0tUqpst-"
      },
      "source": [
        "2.3.2.5 Loading saved model and testing performance on test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fr0kN99FpLtU"
      },
      "outputs": [],
      "source": [
        "def evaluate_test_data_using_saved_model(dataset):\n",
        "  # Load model\n",
        "  try:\n",
        "    model_load_path = working_dir +  \"/best_models/\" + model_checkpoint \n",
        "    loaded_tokenizer = AutoTokenizer.from_pretrained(model_load_path)\n",
        "    loaded_m = (AutoModelForSequenceClassification.from_pretrained(model_load_path, num_labels=3).to(device))\n",
        "    trainer = ProjectTrainer(model=loaded_m)\n",
        "\n",
        "    dataset_encoded = encode_dataset(loaded_tokenizer, dataset)\n",
        "    y_test = np.array(dataset_encoded[\"label\"])\n",
        "    labels = dataset.features[\"label\"].names\n",
        "\n",
        "    test_predictions_all = trainer.predict(dataset_encoded)\n",
        "    y_preds_t = np.argmax(test_predictions_all.predictions, axis=1)\n",
        "    print_classification_report(y_preds_t, y_test, \"for finetuned \" + model_checkpoint)\n",
        "  except FileNotFoundError:\n",
        "    print(\"Could not load a model from the provided path.\")\n",
        "  except Exception as e:\n",
        "    print(\"Could not complete the opration because of the following error: \", e)\n",
        "evaluate_test_data_using_saved_model(project_dataset[\"test\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. LSTM and BiLSTM Section"
      ],
      "metadata": {
        "id": "1F9vTU3fsqvd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3.1 Preparing Data for LSTM and BiLSTM"
      ],
      "metadata": {
        "id": "usNvBXdOsssM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.1.1 Loading"
      ],
      "metadata": {
        "id": "27Lrucz9sxgD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_data_path = os.getcwd() + \"/train_dataset_processed.csv\"\n",
        "testing_data_path = os.getcwd() + \"/test_dataset_processed.csv\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "pqi9lrP8ssKW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "POntTCqot1fd"
      },
      "outputs": [],
      "source": [
        "training_data_df = pd.read_csv(training_data_path, names = [\"Sentiment\", \"Tweet\"])\n",
        "testing_data_df = pd.read_csv(testing_data_path, names = [\"Sentiment\", \"Tweet\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_data_df"
      ],
      "metadata": {
        "id": "z-KULQUruncD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.1.2 Preprocessing "
      ],
      "metadata": {
        "id": "HMj1pJWPuqgW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_punctuations(text):\n",
        "    text = re.sub(r'[^A-Za-z0-9\\s]', '', text, flags=re.MULTILINE)\n",
        "    return text.lower()\n",
        "\n",
        "def remove_stop_words(text):\n",
        "    text = ' '.join([word for word in text.split(\" \") if not word in STOP_WORDS])\n",
        "    return text"
      ],
      "metadata": {
        "id": "EqzHK_pFuo-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_data_df[\"Tweet\"] = training_data_df[\"Tweet\"].apply(str).apply(remove_punctuations).apply(remove_stop_words)\n",
        "testing_data_df[\"Tweet\"] = testing_data_df[\"Tweet\"].apply(remove_punctuations).apply(remove_stop_words)   \n",
        "\n",
        "print(\"Training Data Length: \", len(training_data_df[\"Tweet\"]))\n",
        "print(\"Testing Data Length: \", len(testing_data_df[\"Tweet\"]))\n"
      ],
      "metadata": {
        "id": "BNjR-MD2uuab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count = training_data_df['Sentiment'].value_counts()\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.bar(count.index, count.values)\n",
        "print(training_data_df['Sentiment'].value_counts())"
      ],
      "metadata": {
        "id": "f9dBjpAxuv6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.1.3 Creating vocabulary"
      ],
      "metadata": {
        "id": "CNvxdXtquzjv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter \n",
        "\n",
        "#TRAINING data\n",
        "training_merged_text = ' '.join(training_data_df[\"Tweet\"])\n",
        "training_words = training_merged_text.split()\n",
        "\n",
        "#Using counter method to count all the words\n",
        "training_word_count = Counter(training_words)\n",
        "training_words_total = len(training_words)\n",
        "training_sorted_words = training_word_count.most_common(training_words_total)\n",
        "\n",
        "#TESTING data\n",
        "testing_merged_text = ' '.join(testing_data_df[\"Tweet\"])\n",
        "testing_words = testing_merged_text.split()\n",
        "testing_word_count = Counter(testing_words)\n",
        "testing_words_total = len(testing_words)\n",
        "testing_sorted_words = testing_word_count.most_common(testing_words_total)"
      ],
      "metadata": {
        "id": "KVC0WjT7uxUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert isinstance(training_sorted_words, list), \"Expected training sorted list to be a list\"\n",
        "assert isinstance(training_word_count, Counter), \"Expected training sorted count to be a Counter\"\n",
        "assert training_words_total == 436506, \"Unexpected testing words total\"\n",
        "assert isinstance(testing_sorted_words, list), \"Expected testing sorted list to be a list\"\n",
        "assert isinstance(testing_word_count, Counter), \"Expected training sorted count to be a Counter\"\n",
        "assert testing_words_total == 21531, \"Unexpected testing words total\""
      ],
      "metadata": {
        "id": "uk8b9isQu1hg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.1.4 Convert vocabulary into integer corresponding integer form"
      ],
      "metadata": {
        "id": "zppsxoE8u4gH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_vocabulary_to_integer = {w:i+1 for i, (w,c) in enumerate(training_sorted_words)}\n",
        "testing_vocabulary_to_integer = {w:i+1 for i, (w,c) in enumerate(testing_sorted_words)}"
      ],
      "metadata": {
        "id": "OQpP-bC2u3Kp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert isinstance(training_vocabulary_to_integer, dict), \"Expected integer training vocabulary to be a dictionary\"\n",
        "assert len(training_vocabulary_to_integer) == 43461, \"Unexpected trainig vocabulary total\"\n",
        "assert isinstance(testing_vocabulary_to_integer, dict), \"Expected integer testing vocabulary to be a dictionary\"\n",
        "assert len(testing_vocabulary_to_integer) == 7627, \"Unexpected testing words total\""
      ],
      "metadata": {
        "id": "disO2j5pu7Hf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.1.5 Mapping tweets to integer format based on the generated vocabulary"
      ],
      "metadata": {
        "id": "wDnlP7yOu-60"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_tweets = training_data_df[\"Tweet\"]\n",
        "\n",
        "training_integer_mapping_array = []\n",
        "for tweet in training_tweets:\n",
        "    training_integer_array = [training_vocabulary_to_integer[words] for words in tweet.split()]\n",
        "    training_integer_mapping_array.append(training_integer_array)   \n",
        "\n",
        "training_data_df[\"Integer Mapping\"] = training_integer_mapping_array\n",
        "\n",
        "\n",
        "testing_tweets = testing_data_df[\"Tweet\"]\n",
        "\n",
        "testing_integer_mapping_array = []\n",
        "for tweet in testing_tweets:\n",
        "    testing_integer_array = [testing_vocabulary_to_integer[words] for words in tweet.split()]\n",
        "    testing_integer_mapping_array.append(testing_integer_array)   \n",
        "\n",
        "testing_data_df[\"Integer Mapping\"] = testing_integer_mapping_array\n"
      ],
      "metadata": {
        "id": "JD9_ZLQgu-gl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert isinstance(training_data_df[\"Integer Mapping\"][0], list), \"Expected integer form of training tweets to be a list\"\n",
        "assert isinstance(training_data_df[\"Integer Mapping\"][0][0], int), \"Expected training content to contain int values\"\n",
        "\n",
        "assert isinstance(testing_data_df[\"Integer Mapping\"][0], list), \"Expected integer form of testing tweets vocabulary to be a list\"\n",
        "assert isinstance(testing_data_df[\"Integer Mapping\"][0][0], int), \"Expected testing content to contain int values\""
      ],
      "metadata": {
        "id": "pyfNflnAvDUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.1.6 Analyse tweet length"
      ],
      "metadata": {
        "id": "UkSEfgFevGoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_labels = np.array(training_data_df[\"Sentiment\"])\n",
        "testing_labels = np.array(testing_data_df[\"Sentiment\"])"
      ],
      "metadata": {
        "id": "hKz3x-UAvEzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(training_integer_mapping_array))\n",
        "print(len(testing_integer_mapping_array))"
      ],
      "metadata": {
        "id": "DgVZHjctvIiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_tweet_integer_len = [len(x) for x in training_data_df[\"Integer Mapping\"]]\n",
        "pd.Series(training_tweet_integer_len).hist()\n",
        "plt.show()\n",
        "pd.Series(training_tweet_integer_len).describe()"
      ],
      "metadata": {
        "id": "QFltpDJNvJp2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testing_tweet_integer_len = [len(x) for x in testing_data_df[\"Integer Mapping\"]]\n",
        "pd.Series(testing_tweet_integer_len).hist()\n",
        "plt.show()\n",
        "pd.Series(testing_tweet_integer_len).describe()"
      ],
      "metadata": {
        "id": "vPzAU7NMvK2i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.1.7 Removing tweets with zero length"
      ],
      "metadata": {
        "id": "jEnhClUhvSZq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_integer_mapping_array = [training_integer_mapping_array[i] for i, length in enumerate(training_tweet_integer_len) if length > 0 ]\n",
        "training_labels = [ training_labels[i] for i, length in enumerate(training_tweet_integer_len) if length > 0 ]\n",
        "\n",
        "testing_integer_mapping_array = [testing_integer_mapping_array[i] for i, length in enumerate(testing_tweet_integer_len) if length > 0 ]\n",
        "testing_labels = [ testing_labels[i] for i, length in enumerate(testing_tweet_integer_len) if length > 0 ]"
      ],
      "metadata": {
        "id": "vFS4snGfvMdJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(training_integer_mapping_array))\n",
        "print(len(testing_integer_mapping_array))"
      ],
      "metadata": {
        "id": "60O2PGKBvN8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert len(training_integer_mapping_array) == 27778, \"Unexpected training mapping array length\"\n",
        "assert len(testing_integer_mapping_array) == 1300, \"Unexpected testing mapping array length\""
      ],
      "metadata": {
        "id": "Pir6riidvPYd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.1.8 Padding integer form of tweets"
      ],
      "metadata": {
        "id": "4PJ5eBwevVtY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def padding(integer_mapping_array, sequence_length):\n",
        "    features = np.zeros((len(integer_mapping_array), sequence_length), dtype = int)\n",
        "    for i, tweet_int in enumerate(integer_mapping_array):\n",
        "        tweet_integer_len = len(tweet_int)\n",
        "        \n",
        "        if tweet_integer_len <= sequence_length:\n",
        "            zeroes = list(np.zeros(sequence_length - tweet_integer_len))\n",
        "            new = zeroes + tweet_int\n",
        "        elif tweet_integer_len > sequence_length:\n",
        "            new = tweet_int[0:sequence_length]\n",
        "        \n",
        "        features[i,:] = np.array(new)\n",
        "    \n",
        "    return features"
      ],
      "metadata": {
        "id": "j4iwSDIyvXQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_sequence_length = max(len(length) for length in training_integer_mapping_array)\n",
        "training_integer_mapping_array = padding(training_integer_mapping_array, training_sequence_length)\n",
        "\n",
        "testing_sequence_length = max(len(length) for length in testing_integer_mapping_array)\n",
        "testing_integer_mapping_array = padding(testing_integer_mapping_array, testing_sequence_length)\n",
        "\n",
        "X_test = testing_integer_mapping_array\n",
        "y_test = testing_labels\n",
        "y_test = np.array(y_test)"
      ],
      "metadata": {
        "id": "3_LqBcqUvYYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.1.9 Splitting data into training and validation data"
      ],
      "metadata": {
        "id": "38XxFg6NvaX0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(training_integer_mapping_array, training_labels, test_size=0.3, train_size=0.7)\n",
        "y_train = np.array(y_train)\n",
        "y_valid = np.array(y_valid)"
      ],
      "metadata": {
        "id": "pBqVV8F0vcF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert isinstance(X_train, np.ndarray), \"Expected training features to be numpy array\"\n",
        "assert isinstance(y_train, np.ndarray), \"Expected training labels to be numpy array\"\n",
        "assert isinstance(X_valid, np.ndarray), \"Expected validation features to be numpy array\"\n",
        "assert isinstance(y_valid, np.ndarray), \"Expected validation labels to be numpy array\""
      ],
      "metadata": {
        "id": "fVYEx2NdvdXU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.1.10 Converting datasets into PyTorch data loader objects."
      ],
      "metadata": {
        "id": "mKxWjrfGve56"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create Tensor datasets\n",
        "train_data = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
        "valid_data = TensorDataset(torch.from_numpy(X_valid), torch.from_numpy(y_valid))\n",
        "test_data = TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test))\n",
        "\n",
        "#Batch Size\n",
        "batch_size = 50\n",
        "\n",
        "#Creating data loaders\n",
        "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size, drop_last=True, pin_memory =True)\n",
        "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size, drop_last=True, pin_memory =True)\n",
        "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size, pin_memory =True)"
      ],
      "metadata": {
        "id": "bUcWTJZEvhoa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert isinstance(train_data, torch.utils.data.dataset.TensorDataset), \"Expected training data to be a TensorDataset\"\n",
        "assert len(train_loader) == len(train_data)//batch_size, \"Unexpected training data loader size\"\n",
        "assert isinstance(valid_data, torch.utils.data.dataset.TensorDataset), \"Expected validation data to be a TensorDataset\"\n",
        "assert len(valid_loader) == len(valid_data)//batch_size, \"Unexpected validation data loader size\"\n",
        "assert isinstance(test_data, torch.utils.data.dataset.TensorDataset), \"Expected testing data to be a TensorDataset\"\n",
        "assert len(test_loader) == len(test_data)//batch_size, \"Unexpected testing data loader size\""
      ],
      "metadata": {
        "id": "jDkbWYi1vkhg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.1.11 Obtaining one batch of training data"
      ],
      "metadata": {
        "id": "33pyg4jwvm4V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_data_iterator = iter(train_loader)\n",
        "x_train_sample, y_train_sample = training_data_iterator.next()\n",
        "print('Sample input size: ', x_train_sample.size()) \n",
        "print('Sample input: \\n', x_train_sample)\n",
        "print()\n",
        "print('Sample label size: ', y_train_sample.size())\n",
        "print('Sample label: \\n', y_train_sample)"
      ],
      "metadata": {
        "id": "jGt4Fq2HvnP5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.1.12 Computing class weights to rectify unbalanced classes"
      ],
      "metadata": {
        "id": "VgQVciDuvqRf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classes = np.unique(training_data_df[\"Sentiment\"])\n",
        "class_weights=compute_class_weight(class_weight='balanced', classes=classes, y=training_data_df[\"Sentiment\"])\n",
        "class_weights=torch.tensor(class_weights, dtype=torch.float)"
      ],
      "metadata": {
        "id": "JDWO9rsAvoYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert isinstance(class_weights, torch.Tensor), \"Expected class weights to be a Tensor\"\n",
        "assert len(class_weights) == 3, \"Expected class weights to be 3\""
      ],
      "metadata": {
        "id": "t0ufLAYavr3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3.2 LSTM model"
      ],
      "metadata": {
        "id": "m-LtMNT1vub5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMModel(nn.Module):\n",
        "   def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, num_layers, drop_prob=0.5):\n",
        "        \n",
        "        '''Model initialization - setting up layers'''\n",
        "        super(LSTMModel, self).__init__()\n",
        "\n",
        "        self.output_size = output_size\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "        \n",
        "        #Embedding Layer\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        #LSTM Layer\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, \n",
        "                            dropout=drop_prob, batch_first=True)\n",
        "        \n",
        "        # Dropout Layer\n",
        "        self.dropout = nn.Dropout(0.6)\n",
        "        \n",
        "        # Fully Connected Layer\n",
        "        self.fc = nn.Linear(hidden_dim, output_size)\n",
        "        \n",
        "   def forward(self, x, hidden):\n",
        "        ''' Performing forward pass on input and hidden state '''\n",
        "\n",
        "        batch_size = x.size(0)\n",
        "        x = x.long()\n",
        "\n",
        "        #Embeddings with dropout\n",
        "        embeddings = self.dropout(self.embedding(x))\n",
        "\n",
        "        #LSTM\n",
        "        lstm_out, hidden = self.lstm(embeddings, hidden)\n",
        "\n",
        "        #Stacking up LSTM outputs\n",
        "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
        "        \n",
        "        #Dropout\n",
        "        out = self.dropout(lstm_out)\n",
        "\n",
        "        #Fully Connected\n",
        "        out = self.fc(out)\n",
        "\n",
        "        #Reshaping \n",
        "        output = out.view(batch_size, -1, output_size)\n",
        "\n",
        "        #Getting last batch of labels\n",
        "        output = output[:, -1] \n",
        "        \n",
        "        #Return last output with the hidden state\n",
        "        return output, hidden\n",
        "    \n",
        "    \n",
        "   def init_hidden(self, batch_size):\n",
        "        ''' Initializing hidden state '''\n",
        "        \n",
        "        #Two tensors of size num_layers * batch_size * hidden_dim are initialized to zero\n",
        "        weight = next(self.parameters()).data\n",
        "        hidden = (weight.new(self.num_layers, batch_size, self.hidden_dim).zero_(),\n",
        "                      weight.new(self.num_layers, batch_size, self.hidden_dim).zero_())\n",
        "        \n",
        "        return hidden\n"
      ],
      "metadata": {
        "id": "wfn3hutqvuAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.2.1 Training function"
      ],
      "metadata": {
        "id": "8fAPN7hdv0LI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, epochs, batch_size, train_loader, criterion, clip, valid_loader):\n",
        "  counter = 0\n",
        "  model.train().to(device)\n",
        "\n",
        "  train_losses = list()\n",
        "  validation_losses = list()\n",
        "\n",
        "  #Training loop upto epoch\n",
        "  for epoch in range(epochs): \n",
        "\n",
        "      #Hidden state initialization\n",
        "      hidden = model.init_hidden(batch_size)\n",
        "\n",
        "      #Batch wise looping\n",
        "      for inputs, labels in train_loader:\n",
        "          \n",
        "          counter += 1\n",
        "          \n",
        "          #Recreating variables for the hidden state to avoid backpropagate\n",
        "          hidden = tuple([each.data for each in hidden])\n",
        "\n",
        "          #Setting the gradients of optimized torch\n",
        "          model.zero_grad()\n",
        "\n",
        "          inputs = inputs.to(device)\n",
        "          labels = labels.to(device)\n",
        "\n",
        "          #Getting the prediction from the model\n",
        "          prediction, hidden = model(inputs, hidden)\n",
        "          \n",
        "          #Calculating training Loss\n",
        "          loss = criterion(prediction.squeeze(), labels.long())\n",
        "\n",
        "          loss.backward()\n",
        "          \n",
        "          #Gradient Clipping\n",
        "          nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "          optimizer.step()\n",
        "\n",
        "          #Calculating validation loss for each epoch\n",
        "          if counter % print_step == 0:\n",
        "\n",
        "              #Hidden state initialization\n",
        "              valid_hidden_state = model.init_hidden(batch_size)\n",
        "              val_losses = []\n",
        "\n",
        "              model.eval()\n",
        "\n",
        "              for inputs, labels in valid_loader:\n",
        "\n",
        "                  #Recreating variables for the hidden state to avoid backpropagate\n",
        "                  valid_hidden_state = tuple([each.data for each in valid_hidden_state])\n",
        "\n",
        "                  inputs = inputs.to(device)\n",
        "                  labels = labels.to(device)\n",
        "\n",
        "                  #Getting the prediction from the model\n",
        "                  prediction, valid_hidden_state = model(inputs, valid_hidden_state)\n",
        "\n",
        "                  #Calculating validation loss for each epoch\n",
        "                  valid_loss = criterion(prediction.squeeze(), labels.long())\n",
        "                  val_losses.append(valid_loss.item())\n",
        "\n",
        "              model.train()\n",
        "              \n",
        "              train_losses.append(loss.item())\n",
        "              validation_losses.append(np.mean(val_losses))\n",
        "\n",
        "              print(\"Epoch: {}/{}...\".format(epoch + 1, epochs),\n",
        "                    \"Step: {}...\".format(counter),\n",
        "                    \"Training Loss: {:.6f}...\".format(loss.item()),\n",
        "                    \"Validation Loss: {:.6f}\".format(np.mean(validation_losses)))\n",
        "                          \n",
        "  return train_losses, validation_losses"
      ],
      "metadata": {
        "id": "C9fQxVARvyma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.2.2 Setting up hyperparameters"
      ],
      "metadata": {
        "id": "_JqACrJIv3-V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(training_vocabulary_to_integer) + 1      #Vocabulary size for embedding layer\n",
        "output_size = 3                                           #Output size\n",
        "embedding_dim = 200                                       #Embedding dimension\n",
        "hidden_dim = 256                                          #Hidden dimension\n",
        "num_layers = 2                                            #Number of layers\n",
        "lstm_model = LSTMModel(vocab_size, output_size, embedding_dim, hidden_dim, num_layers).to(device)       #LSTM Object\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(class_weights).to(device)                       #Categorical cross entropy\n",
        "\n",
        "counter = 0\n",
        "epochs = 100                                              #Number of epochs\n",
        "print_step = 100\n",
        "clip = 10                                                 #For gradient clipping\n",
        "lr = 1e-5                                                 #Learning Rate\n",
        "optimizer = torch.optim.Adam(lstm_model.parameters(), lr=lr)   #Optimization Algorithm"
      ],
      "metadata": {
        "id": "guDO21IPv2Di"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert isinstance(lstm_model, LSTMModel), \"Expected model to be LSTMModel\"\n",
        "assert isinstance(lstm_model.lstm, nn.LSTM), \"Expected lstm layer to be nn.LSTM\"\n",
        "assert isinstance(lstm_model.embedding, nn.Embedding), \"Expected embedding layer to be nn.Embedded\"\n",
        "assert isinstance(lstm_model.lstm, nn.LSTM), \"Expected lstm layer to be nn.LSTM\"\n",
        "assert isinstance(lstm_model.dropout, nn.Dropout), \"Expected dropout layer to be nn.Dropout\"\n",
        "assert isinstance(lstm_model.fc, nn.Linear), \"Expected fc layer to be nn.Linear\""
      ],
      "metadata": {
        "id": "Rczcgu-9v5uD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.2.3 Training phase"
      ],
      "metadata": {
        "id": "rLG_T9zev9Q2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_train_losses, lstm_validation_losses = train(lstm_model, epochs, batch_size, train_loader, criterion, clip, valid_loader);"
      ],
      "metadata": {
        "id": "XM8RBK48v7EA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.2.4 Training and Validation plot"
      ],
      "metadata": {
        "id": "Vq583k83wCSg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(lstm_train_losses, color=\"blue\")\n",
        "plt.plot(lstm_validation_losses, color=\"red\")\n",
        "plt.ylim(-0.5, 2)\n",
        "plt.xlabel(\"Loss every 50 steps\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.legend([\"train_loss\", \"validation_loss\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qVG1Ps24v_Az"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.2.5 Saving LSTM Model"
      ],
      "metadata": {
        "id": "7j5E80ddwGXZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from joblib import dump, load\n",
        "\n",
        "model_checkpoint = \"lstm\"\n",
        "working_dir = os.getcwd() + \"/best_models\"\n",
        "\n",
        "if not os.path.exists(working_dir + \"/\" + model_checkpoint + \"/output-\" + model_checkpoint):\n",
        "  os.makedirs(working_dir + \"/\" + model_checkpoint)\n",
        "lstm_model_output_dir = working_dir + \"/\" + model_checkpoint + \"/output-\" + model_checkpoint + \".joblib\"\n",
        "\n",
        "dump(lstm_model, lstm_model_output_dir)"
      ],
      "metadata": {
        "id": "mzNb0rsiwJAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.2.6 Testing function"
      ],
      "metadata": {
        "id": "E5H_8efnwMRR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def testing(model, test_loader, batch_size):\n",
        "  test_losses = []\n",
        "  y_pred = []\n",
        "  y_true = []\n",
        "\n",
        "  num_correct = 0\n",
        "\n",
        "  #Hidden state initialization\n",
        "  hidden = model.init_hidden(batch_size)\n",
        "  model.eval()\n",
        "\n",
        "  #Batch wise looping\n",
        "  for inputs, labels in test_loader:\n",
        "\n",
        "      #Recreating variables for the hidden state to avoid backpropagate\n",
        "      hidden = tuple([each.data for each in hidden])\n",
        "\n",
        "      inputs, labels = inputs.to(device), labels.to(device)\n",
        "      \n",
        "      output, hidden = model(inputs, hidden)\n",
        "      \n",
        "      #Calculating test loss\n",
        "      test_loss = criterion(output.squeeze(), labels.long())\n",
        "      test_losses.append(test_loss.item())\n",
        "\n",
        "      \n",
        "      #Converting probabilities to output classse\n",
        "      pred = torch.argmax(output ,dim=1, keepdim=True)\n",
        "      y_pred.extend(pred.cpu().detach().numpy())        #Saving predicted values\n",
        "      y_true.extend(np.array(labels.long().cpu()))      #Saving true values\n",
        "\n",
        "  return y_pred, y_true"
      ],
      "metadata": {
        "id": "cO39_O3HwKkC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.2.7 Testing Phase"
      ],
      "metadata": {
        "id": "eBtnoXpTwORH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_y_pred, lstm_y_true = testing(lstm_model, test_loader, batch_size = batch_size)"
      ],
      "metadata": {
        "id": "B_49aO3NwRTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.2.8 Results"
      ],
      "metadata": {
        "id": "m_WZRnwKwVTH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print_classification_report(lstm_y_pred, lstm_y_true, title = \"LSTM\")"
      ],
      "metadata": {
        "id": "zXGPlKV4wTo3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3.3. BiLSTM model"
      ],
      "metadata": {
        "id": "zAVyAggHwZlE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BiLSTMModel(nn.Module):\n",
        "   def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, num_layers, drop_prob=0.7):\n",
        "        \n",
        "        '''Model initialization - setting up layers'''\n",
        "        super(BiLSTMModel, self).__init__()\n",
        "\n",
        "        self.output_size = output_size\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "        \n",
        "        #Embedding Layer\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        #BiLSTM Layer\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers = num_layers, \n",
        "                            dropout=drop_prob, batch_first=True, bidirectional=True)\n",
        "        \n",
        "        # Dropout Layer\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        \n",
        "        # Fully Connected Layer\n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_size)\n",
        "        \n",
        "   def forward(self, x, hidden):\n",
        "        ''' Performing forward pass on input and hidden state '''\n",
        "\n",
        "        batch_size = x.size(0)\n",
        "        x = x.long()\n",
        "\n",
        "        #Embeddings with dropout\n",
        "        embeddings = self.dropout(self.embedding(x))\n",
        "\n",
        "        #BiLSTM\n",
        "        lstm_out, hidden = self.lstm(embeddings, hidden)\n",
        "\n",
        "        #Stacking up BiLSTM outputs\n",
        "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim * 2)\n",
        "        \n",
        "        #Dropout\n",
        "        out = self.dropout(lstm_out)\n",
        "\n",
        "        #Fully Connected\n",
        "        out = self.fc(out)\n",
        "\n",
        "        #Reshaping \n",
        "        output = out.view(batch_size, -1, output_size)\n",
        "\n",
        "        #Getting last batch of labels\n",
        "        output = output[:, -1] \n",
        "        \n",
        "        #Return last output with the hidden state\n",
        "        return output, hidden\n",
        "    \n",
        "    \n",
        "   def init_hidden(self, batch_size):\n",
        "        ''' Initializing hidden state '''\n",
        "        \n",
        "        #Two tensors of size num_layers * batch_size * hidden_dim are initialized to zero\n",
        "        weight = next(self.parameters()).data\n",
        "        hidden = (weight.new(self.num_layers * 2, batch_size, self.hidden_dim).zero_(),\n",
        "                      weight.new(self.num_layers * 2, batch_size, self.hidden_dim).zero_())\n",
        "        \n",
        "        return hidden\n"
      ],
      "metadata": {
        "id": "c6PhaeZLwXnK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.3.1 Setting up hyperparameters"
      ],
      "metadata": {
        "id": "U-GNcKj8wd_o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(training_vocabulary_to_integer) + 1      #Vocabulary size for embedding layer\n",
        "output_size = 3                                           #Output size\n",
        "embedding_dim = 200                                       #Embedding dimension\n",
        "hidden_dim = 256                                          #Hidden dimension\n",
        "num_layers = 2                                            #Number of layers\n",
        "bilstm_model = BiLSTMModel(vocab_size, output_size, embedding_dim, hidden_dim, num_layers).to(device)       #LSTM Object\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(class_weights).to(device)                       #Categorical cross entropy\n",
        "\n",
        "counter = 0\n",
        "epochs = 100                                              #Number of epochs\n",
        "print_step = 100\n",
        "clip = 10                                                 #For gradient clipping\n",
        "lr = 1e-6                                                 #Learning Rate\n",
        "optimizer = torch.optim.Adam(bilstm_model.parameters(), lr=lr)   #Optimization Algorithm"
      ],
      "metadata": {
        "id": "oJDwcWUywedk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert isinstance(bilstm_model, BiLSTMModel), \"Expected model to be LSTMModel\"\n",
        "assert isinstance(bilstm_model.lstm, nn.LSTM), \"Expected lstm layer to be nn.LSTM\"\n",
        "assert isinstance(bilstm_model.embedding, nn.Embedding), \"Expected embedding layer to be nn.Embedded\"\n",
        "assert isinstance(bilstm_model.lstm, nn.LSTM), \"Expected lstm layer to be nn.LSTM\"\n",
        "assert isinstance(bilstm_model.dropout, nn.Dropout), \"Expected dropout layer to be nn.Dropout\"\n",
        "assert isinstance(bilstm_model.fc, nn.Linear), \"Expected fc layer to be nn.Linear\""
      ],
      "metadata": {
        "id": "OcZ6MFPqwcLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.3.2 Training Phase"
      ],
      "metadata": {
        "id": "5DetAC_Uwiov"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bilstm_train_losses, bilstm_validation_losses = train(bilstm_model, epochs, batch_size, train_loader, criterion, clip, valid_loader);"
      ],
      "metadata": {
        "id": "b5RMZm6HwjRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.3.3 Training and Validation plot"
      ],
      "metadata": {
        "id": "vM22bS_iwp8M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(bilstm_train_losses, color=\"blue\")\n",
        "plt.plot(bilstm_validation_losses, color=\"red\")\n",
        "plt.ylim(-0.5, 2)\n",
        "plt.xlabel(\"Loss every 50 steps\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.legend([\"train_loss\", \"validation_loss\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "W8WB-_b8wl23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.3.4 Saving BiLSTM model"
      ],
      "metadata": {
        "id": "JAWWxW8ewt89"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from joblib import dump, load\n",
        "\n",
        "model_checkpoint = \"bilstm\"\n",
        "working_dir = os.getcwd() + \"/best_models/\" + model_checkpoint \n",
        "\n",
        "if not os.path.exists(working_dir + \"/output-\" + model_checkpoint):\n",
        "  os.makedirs(working_dir)\n",
        "bilstm_model_output_dir = working_dir + \"/output-\" + model_checkpoint + \".joblib\"\n",
        "\n",
        "dump(bilstm_model, bilstm_model_output_dir)"
      ],
      "metadata": {
        "id": "_8znVSDGwryw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.3.5 Testing Phase"
      ],
      "metadata": {
        "id": "3jQ39edrwxwV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bilstm_y_pred, bilstm_y_true = testing(bilstm_model, test_loader, batch_size = batch_size)"
      ],
      "metadata": {
        "id": "nhdZ-SiHwxWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.3.6 Results"
      ],
      "metadata": {
        "id": "O1xKDholw0Km"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print_classification_report(bilstm_y_pred, bilstm_y_true, title = \"BiLSTM\")"
      ],
      "metadata": {
        "id": "JLDy5qWawz2M"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "W22_COMP6321_Project.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}